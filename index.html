<script src="http://www.google.com/jsapi" type="text/javascript"></script> 
<script type="text/javascript">google.load("jquery", "1.3.2");</script>

<script type="text/javascript" id="MathJax-script" async
          src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml.js">
        </script>
        <!-- sets $$ as shorthand for in-text math functions -->
        <script>
         window.MathJax = {
            tex: {
              inlineMath: [['$', '$'], ['\\(', '\\)']]
            }
          };
        </script>

<style type="text/css">
	body {
		font-family: "HelveticaNeue-Light", "Helvetica Neue Light", "Helvetica Neue", Helvetica, Arial, "Lucida Grande", sans-serif; 
		font-weight:300;
		font-size:18px;
		margin-left: auto;
		margin-right: auto;
		width: 1100px;
	}
	
	h1 {
		font-size:32px;
		font-weight:300;
	}
	
	.disclaimerbox {
		background-color: #eee;		
		border: 1px solid #eeeeee;
		border-radius: 10px ;
		-moz-border-radius: 10px ;
		-webkit-border-radius: 10px ;
		padding: 20px;
	}

	video.header-vid {
		height: 140px;
		border: 1px solid black;
		border-radius: 10px ;
		-moz-border-radius: 10px ;
		-webkit-border-radius: 10px ;
	}
	
	img.header-img {
		height: 140px;
		border: 1px solid black;
		border-radius: 10px ;
		-moz-border-radius: 10px ;
		-webkit-border-radius: 10px ;
	}
	
	img.rounded {
		border: 1px solid #eeeeee;
		border-radius: 10px ;
		-moz-border-radius: 10px ;
		-webkit-border-radius: 10px ;
	}
	
	a:link,a:visited
	{
		color: #1367a7;
		text-decoration: none;
	}
	a:hover {
		color: #208799;
	}
	
	td.dl-link {
		height: 160px;
		text-align: center;
		font-size: 22px;
	}
	
	.layered-paper-big { /* modified from: http://css-tricks.com/snippets/css/layered-paper/ */
		box-shadow:
		0px 0px 1px 1px rgba(0,0,0,0.35), /* The top layer shadow */
		5px 5px 0 0px #fff, /* The second layer */
		5px 5px 1px 1px rgba(0,0,0,0.35), /* The second layer shadow */
		10px 10px 0 0px #fff, /* The third layer */
		10px 10px 1px 1px rgba(0,0,0,0.35), /* The third layer shadow */
		15px 15px 0 0px #fff, /* The fourth layer */
		15px 15px 1px 1px rgba(0,0,0,0.35), /* The fourth layer shadow */
		20px 20px 0 0px #fff, /* The fifth layer */
		20px 20px 1px 1px rgba(0,0,0,0.35), /* The fifth layer shadow */
		25px 25px 0 0px #fff, /* The fifth layer */
		25px 25px 1px 1px rgba(0,0,0,0.35); /* The fifth layer shadow */
		margin-left: 10px;
		margin-right: 45px;
	}

	.paper-big { /* modified from: http://css-tricks.com/snippets/css/layered-paper/ */
		box-shadow:
		0px 0px 1px 1px rgba(0,0,0,0.35); /* The top layer shadow */

		margin-left: 10px;
		margin-right: 45px;
	}


	.layered-paper { /* modified from: http://css-tricks.com/snippets/css/layered-paper/ */
		box-shadow:
		0px 0px 1px 1px rgba(0,0,0,0.35), /* The top layer shadow */
		5px 5px 0 0px #fff, /* The second layer */
		5px 5px 1px 1px rgba(0,0,0,0.35), /* The second layer shadow */
		10px 10px 0 0px #fff, /* The third layer */
		10px 10px 1px 1px rgba(0,0,0,0.35); /* The third layer shadow */
		margin-top: 5px;
		margin-left: 10px;
		margin-right: 30px;
		margin-bottom: 5px;
	}
	
	.vert-cent {
		position: relative;
		top: 50%;
		transform: translateY(-50%);
	}
	
	hr
	{
		border: 0;
		height: 1px;
		background-image: linear-gradient(to right, rgba(0, 0, 0, 0), rgba(0, 0, 0, 0.75), rgba(0, 0, 0, 0));
	}

  figure 
  {
    display: inline-block;
  }

  figure figcaption 
  {
	margin-top: .5em;
    border: 1px;
    text-align: center;
	font-weight: bold;
  }

  .column {
            float: left;
            height: auto;
            padding: 5px;
        }

</style>

<html>
<head>
	<title>OpenGIT</title>
	<meta property="og:image" content="Path to my teaser.png"/> <!-- Facebook automatically scrapes this. Go to https://developers.facebook.com/tools/debug/ if you update and want to force Facebook to rescrape. -->
	<meta property="og:title" content="Creative and Descriptive Paper Title." />
	<meta property="og:description" content="Paper description." />

	<!-- Get from Google Analytics -->
	<!-- Global site tag (gtag.js) - Google Analytics -->
	<script async src=""></script> 
	<script>
		window.dataLayer = window.dataLayer || [];
		function gtag(){dataLayer.push(arguments);}
		gtag('js', new Date());

		gtag('config', 'UA-75863369-6');
	</script>
</head>

<body>
	<br>
	<center>
		<span style="font-size:36px">Open-Domain Compositional Image Editing with Text</span>
		<table align=center width=600px>
			<table align=center width=600px>
				<tr>
					<td align=center width=100px>
						<center>
							<span style="font-size:24px"><a href="https://en.wikipedia.org/wiki/James_J._Gibson">Zhizhuo Zhou</a></span>
						</center>
					</td>
          <td align=center width=100px>
						<center>
							<span style="font-size:24px"><a href="https://en.wikipedia.org/wiki/James_J._Gibson">Gaoyue Zhou</a></span>
						</center>
					</td>
					<td align=center width=100px>
						<center>
							<span style="font-size:24px"><a href="https://www.linkedin.com/in/qiyun-chen-313a16193/">Qiyun Chen</a></span>
						</center>
					</td>
					
				</tr>
			</table>
			<table align=center width=250px>
				<tr>
					<td align=center width=120px>
						<center>
							<span style="font-size:24px"><a href='https://colab.research.google.com/drive/15Gtaa_G2q1MbiGzX1bySxh1khYFNLd-0?usp=sharing'>[Demo]</a></span>
						</center>
					</td>
					<td align=center width=120px>
						<center>
							<span style="font-size:24px"><a href='https://docs.google.com/presentation/d/1nU3pmIGNv1MxyCLy85ulhunM9RTHftcT2orRO4DE5XI/edit?usp=sharing'>[Slides]</a></span><br>
						</center>
					</td>
				</tr>
			</table>
		</table>
	</center>

	<center>
		<table align=center width=850px>
			<tr>
				<td width=260px>
					<center>
						<img class="round" style="width:600px" src="./resources/teaser.png"/>
					</center>
				</td>
			</tr>
		</table>
		<table align=center width=850px>
			<tr>
				<td>
					This is a final course project for <a href="https://learning-image-synthesis.github.io/sp22/">16-726 Learning Based Image Synthesis</a> in Spring 2022</a>.
				</td>
			</tr>
		</table>
	</center>

	<hr>

	<table align=center width=850px>
		<center><h1>Abstract</h1></center>
		<tr>
			<td>
				Building on top of a recent class conditional generative image transformer, <a href="https://arxiv.org/abs/2202.04200">MaskGIT</a>, we propose to develop a pipeline for language controlled image editing. We focus 
				on a simple yet ubiquitous logic, {a} to {b}, which can be expressed as "change {a} to {b}" or "replace {a} with {b}", and apply the desired actions by performing edits on 
				an input image. We constrain {a} to be a class in <a href="https://cocodataset.org/">COCO</a> and {b} to be an open-domain phrase that describes an object. 
        Finally, we leverage <a href="https://arxiv.org/abs/1703.06870">Mask R-CNN</a>, <a href="https://arxiv.org/abs/2202.04200">MaskGIT</a>, 
        and <a href="https://openai.com/blog/clip/">CLIP</a> to perform region targeted class conditional image generation
        and image-language optimization.
			</td>
		</tr>
	</table>

	<br>
	<hr>
	
	<!-- <table align=center width=850px>
		<center><h1>Related Works </h1></center>
		<tr>
			<td>
				Recent works in learning based synthesis show that a two stage VQ-VAE and transformer setup is able to generate realistic and diverse images. 
        
			</td>
		</tr>
	</table> -->

	<!-- <br>
	<hr>
	<br> -->

	<table align=center width=850px>
		<center><h1>Method</h1></center>
		<tr>
      <td>
        <center>
          <img class="round" style="width:800px" src="./resources/pipeline.png"/>
          <figcaption>OpenGIT Pipeline</figcaption>
        </center>
			</td>
		</tr>
	</table>

  <table align=center width=850px>
    <tr>
      <td>
        <br>
        Our proposed OpenGIT Pipeline consists of three main components: <b>Natural Language Processing Module</b>,
        <b>Class Conditional Image Generation</b>, and <b>CLIP Guided Optimization</b>.<br><br>
        The entire procedure is as follows: <br><br>
        Firstly, OpenGIT performs Natural Language Processing (NLP), the user inputs an image and a natural language prompt
        specifying the source category (the item to be replaced in the input image) and the target category (
        the item to change into). The source category must be from one of the 
        <a href="https://github.com/amikelive/coco-labels"> categories in the COCODataset </a>. The main target
        category (or categories) should be within the 1000 lables in the <a href="https://www.image-net.org/">ImageNet Dataset</a>. 
        However, it can be a simple category like "husky", or it can also be any open-domain phrase that describes
        the category (categories), such as "zebra-striped cat" or "fish and strawberry".<br>
        As an illustration for the first module, the user could input an image of a dog and a prompt 
        "Replace dog with lemon husky". Then, our module would parse "dog" as our source category, "husky"
        as our target category, and "lemon husky" as our CLIP prompt. Here, note that because we used a descriptive
        word for our target category, we would use CLIP to further optimize our image. Otherwise, if the input prompt
        merely said "Replace dog with husky", then we would not have any CLIP prompt nor perform CLIP optimization.
        <br><br>
        Secondly, OpenGIT performs Class Conditional Generation. This module take as inputs
        the input image along with the source and target category. The module first uses Detectron2 and Mask R-CNN
        to locate the bounding box of the source category, and then feed it through a pretrained MaskGIT
        model to generate a class conditioned image. Here, we offer users to choose a hyperparameter 
        $r \in (0, 1]$ as the scale of the original bounding box. For example, in general, if $r = 1$, 
        then OpenGIT would completely replace the source category with the target category. If $r < 1$, 
        then OpenGIT would keep the outerior of the source category and merges the target category into the 
        source category (see examples in Results section below).
        <br><br>
        Thirdly, OpenGIT performs CLIP Guided Optimization. For the user input prompts with a descriptive
        prompt for the target category, the first NLP module would obtain a non-empty CLIP prompt.
        We take our class conditioned generated output and the CLIP prompt as the input to this
        module, and we use a pretrained CLIP module to generate the output image.
      </td>
    </tr>
  </table>

  <br>
  <hr>
  <br>

  <table align=center width=850px>
		<center><h1>Modules</h1></center>
		<tr>
			<td>
				<h3>Object Detection</h3>
				<center>
					<img class="round" style="width:800px" src="./resources/m1_maskrcnn.png"/>
					<figcaption>MaskRCNN Pipeline</figcaption>
				</center>
				<p>Once we get the object class to be replaced by parsing the user prompt, we use a MaskRCNN model pretained on the <a href="https://cocodataset.org/">COCO</a> dataset to predict the bounding box of the specified object in the input image. The above figure shows the model structure of MaskRCNN. MaskRCNN is an extension of Faster R-CNN with an additional branch for predicting segmentation masks on each region of interest. The model has two stages. During the first stage, regions of interest are proposed by the region proposal network. These will be further processed by the Region of Interest Pooling networks to exact features and perform object classification. In parallel to object classification, binary masks of objects are predicted by the Mask Head portion denoted in the figure. The outputs of MaskRCNN include class labels, bounding boxes, and object masks for all objects. In OpenGIT, we make use of the bounding box of the specified object for later processing. </p>
				<br><br>
			
			</td>
		</tr>
		<tr>
			<td>
				<h3>Class-Conditioned Image Editing</h3>
				<center>
					<img class="round" style="width:800px" src="./resources/m2_maskgit.png"/>
					<figcaption>MaskGIT Pipeline</figcaption>
				</center>
				<p>
          Given an input image, target label and a bounding box automatically produced by 
          the previous MaskRCNN Pipeline, we use a pretrained MaskGIT model
          for class-conditioned image editing. The region inside of the bounding box
          is considered as the initial mask to the MaskGIT iterative decoding algorithm.
          <br><br>
          The iterative decoding algorithm runs T steps. It starts with the initial mask,
          in our case, it's the rectangle gray area inside of the bounding box. 
          Let the masked tokens be $Y_M^{(t)}$.
          At each step t, the model produces the image via predicting all tokens
          at the same time but only preserving the most confident ones. 
          We define each masked location as $i$. A token $y_i^{(t)}$ is sampled 
          based on its prediction probabilities $p_i^{(t)} \in \mathbf{R}^K$.
          The confidence score is defined as the corresponding prediction score 
          of a token $y_i^{(t)}$, and it indicates how confident our model is of this
          prediction. For the non-masked regions, the confidence score is set to 1.0.

          For the less confident tokens, they are considered as the mask input to the next iteration,
          the mask ratio is designed to be decreasing so that within T iterations,
          all tokens are being generated. 
          <br><br>
          After we perform this decoding algorithm for T iterations, 
          the model generates a resulting image that has contents of the given target label 
          and contexts of the input image.<br><br>
          In our example above, we see that given our original input image of a dog,
          target label <i>cat</i> and a bounding box, 
          the contents within a box becomes the initial mask, and we obtain a cat image 
          with the contexts (e.g. fur and color) that's similar to the original dog image.
          <br>




		</p>
		<br><br>
			</td>
		</tr>
		<tr>
			<td>
				<h3>Fine-tuning with Text</h3>
				<center>
					<img class="round" style="width:800px" src="./resources/m3_clip.png"/>
					<figcaption>CLIP Pipeline</figcaption>
				</center>
				<p>Given a sample of class-conditioned image $x$, let $x = f_d(q_z)$ where $f_d$ is the VQ-VAE decoder and $q_z$ is the grid of quantized latent vectors. We optimize 
					the text image similarity loss using pretrained CLIP model, CLIP($f_d(q_z)$, text_prompt). By fixing the decoder $f_d$, we optimize the text image similarity loss
					by performing gradient descent steps in $q_z$ space. This allows us to fine-tune the generated image on an open-domain text prompt. We see that pretrained CLIP 
					model provides especially good guidance when we want to modify the color or texture of the generated images. However, if if we want to generate a new object 
					from scratch, performing gradient descent on $q_z$ w.r.t. to CLIP loss does not lead to good results. In practice, we rely on pretrained MaskGIT to do content and object 
					level image editing and we use CLIP optimization to control the texture of the output. When combined, MaskGIT + CLIP can produce a lot of creative edits in a huge space 
					that is effectively 1,000 ImageNet classes * millions of potential texture and style adjective text prompts. 
				</p>
				<br>
			</td>
		</tr>
	</table>

  

	<br>
	<hr>
	<br>

	<table align=center width=850px>
		<center><h1>Ablation</h1></center>
		<tr>
			<td>
				<h3>The Effect of Bounding Box Size</h3>
				<p>While generating edited images with OpenGIT, we find that the output is particularly sensitive to the size of the bounding box, even if the bounding box always bounds the same object. Therefore, we introduce another parameter, $r$, the ratio of the size of the desired bounding box to that of the original bounding box given by MaskRCNN as described above. While rescaling the bounding box, we make sure both the center and the aspect ratio of the original bounding box are preserved. Here are some results produced by OpenGIT when we specify different $r$ while keeping other inputs the same.</p>
				<table align=center width=850px>
					<div class="row">
					  <div class="column">
						<figure>
						  <img src="results/bbox_ratio/original_05-08-12-15-47_t332_happy_r0.4.jpeg" height=120px>
						  <figcaption>User Input Image: cat</figcaption>
						</figure>
					  </div>
					  <div class="column">
						<figure>
						  <img src="results/bbox_ratio/bbox04.png" height=120px>
						  <figcaption>Bounding box $r = 0.4$</figcaption>
						</figure>
					  </div>
					  <div class="column">
						<figure>
						  <img src="results/bbox_ratio/before-clip_05-08-12-15-47_t332_happy_r0.4.jpg" height=120px>
						  <figcaption>Output </figcaption>
						</figure>
					  </div>
					</div>
		  
					<div class="row">
					  <div class="column">
						<figure>
						  <img src="results/bbox_ratio/original_05-08-12-19-27_t332_happy_r1.jpeg" height=120px>
						  <figcaption>User Input Image: cat</figcaption>
						</figure>
					  </div>
					  <div class="column">
						<figure>
						  <img src="results/bbox_ratio/bbox1.png" height=120px>
						  <figcaption>Bounding box $r = 1$</figcaption>
						</figure>
					  </div>
					  <div class="column">
						<figure>
						  <img src="results/bbox_ratio/before-clip_05-08-12-18-40_t332_happy_r1.jpg" height=120px>
						  <figcaption>Output</figcaption>
						</figure>
					</div>
					</div>

				</table>
				<p>To generate the above visualizations, we give the same prompt "change cat to rabbit". For the first example, when $r = 0.4$, the bounding box only covers the center part of the cat's face. It's expected that the model would produce a rabbit face that merges well with the original image. When $r = 1$, the whole cat is covered in the bounding box. In this case, the model replaces the cat with a rabbit, including ears and the body. Since there is no inherent quality difference between the two ways of editing and it should be up to the user to decide which output gives the desired image editing effect, we let users specify $r$ directly in our interface. Future work includes inferring $r$ automatically given descriptive words like "face only", "partially", "completely", etc.</p>

			</td>
		</tr>
	</table>

  

	<br>
	<hr>
	<br>

	<table align=center width=850px>
		<center><h1>Results</h1></center>
		<tr>
			<td>
				Here're the results of our OpenGIT pipeline. 
        Recall that the user inputs an image of an object with a prompt 
        similar to "Replace {a} with {b}".<br><br>
        Firstly, we present the image generation for the case where {b} is simply a target category, 
        thus no CLIP optimization is performed.
        <table align=center width=850px>
          <div class="row">
            <div class="column">
              <figure>
                <img src="inputs/orig_dog.png" width=320px>
                <figcaption>User Input Image: dog</figcaption>
              </figure>
            </div>
            <div class="column">
              <figure>
                <img src="results/16726_dog_leopard.jpg" width=320px>
                <figcaption>Replace dog with leopard </figcaption>
              </figure>
            </div>
          </div>

          <div class="row">
            <div class="column">
              <figure>
                <img src="inputs/keyboard.jpeg" width=320px>
                <figcaption>User Input Image: keyboard</figcaption>
              </figure>
            </div>
            <div class="column">
              <figure>
                <img src="results/16726_keyboard_cat.png" width=320px>
                <figcaption>Replace keyboard with cat</figcaption>
              </figure>
            </div>
            <div class="column">
              <figure>
                <img src="inputs/dog.jpeg" width=320px>
                <figcaption>User Input Image: dog</figcaption>
              </figure>
            </div>
            <div class="column">
              <figure>
                <img src="results/16726_dog_husky.jpg" width=320px>
                <figcaption>Replace dog with husky</figcaption>
              </figure>
            </div>
          </div>

          <div class="column">
            <figure>
              <img src="inputs/boat.jpeg" width=320px>
              <figcaption>User Input Image: boat</figcaption>
            </figure>
          </div>
        </div>

        <div class="column">
          <figure>
            <img src="results/16726_boat_loaf.jpg" width=320px>
            <figcaption>Replace dog with french loaf</figcaption>
          </figure>
        </div>
      </div>
      </table>
      
      Next, we present the image generation for the case where {b} is a descriptive phrase
      about a target category, we perform CLIP optimization in this case.

      <table align=center width=850px>
        <div class="row">
          <div class="column">
            <figure>
              <img src="inputs/orig_dog.png" width=320px>
              <figcaption>User Input Image: dog</figcaption>
            </figure>
          </div>
          <div class="column">
            <figure>
              <img src="results/purple_butterfly_leopard.png" width=320px>
              <figcaption>Replace dog with purple butterfly leopard</figcaption>
            </figure>
          </div>
        </div>

          <div class="column">
            <figure>
              <img src="inputs/keyboard.jpeg" width=320px>
              <figcaption>User Input Image: keyboard</figcaption>
            </figure>
          </div>
          <div class="column">
            <figure>
              <img src="results/keyboard_zebra_cat.jpg" width=320px>
              <figcaption>Replace keyboard with zebra-striped cat</figcaption>
            </figure>
          </div>
          <div class="column">
            <figure>
              <img src="inputs/dog.jpeg" width=320px>
              <figcaption>User Input Image: dog</figcaption>
            </figure>
          </div>
          <div class="column">
            <figure>
              <img src="results/dog_rainbow_husky.jpg" width=320px>
              <figcaption>Replace dog with rainbow husky</figcaption>
            </figure>
          </div>
        </div>

        <div class="column">
          <figure>
            <img src="inputs/boat.jpeg" width=320px>
            <figcaption>User Input Image: boat</figcaption>
          </figure>
        </div>
      </div>

      <div class="column">
        <figure>
          <img src="results/boat_cinnamon_roll.jpg" width=320px>
          <figcaption>Replace boat with cinnamon roll</figcaption>
        </figure>
      </div>
    </div>
    </table>

    Lastly, we present the results of {b} where b is a combination of different categories.
    Essentially, we achieve the results via combining the latent features of different categories.
    <table align=center width=850px>
      <div class="row">
        <div class="column">
          <figure>
            <img src="inputs/orig_dog.png" width=320px>
            <figcaption>User Input Image: dog</figcaption>
          </figure>
        </div>
        <div class="column">
          <figure>
            <img src="results/dog_fish_n_strawberry.png" width=320px>
            <figcaption>Replace dog with fish and strawberry</figcaption>
          </figure>
        </div>
      </div>
    </table>


			</td>
		</tr>
	</table>

	<br>
	<hr>
	<br>
	<h1>Additional Results</h1>
	<table align=center width=850px>
	We show some results where we condition on more abstract adjectives, such as "angry", "cute", and "beautiful." We see that after CLIP optimization, the visual quality 
	of the image goes down. This makes sense since we are now moving away from the quantized q_z that the decoder is familiar with generating images from, and instead, feeding it
	non-quantized q_z. New works such as DALL-E 2 show that a diffusion based generator may be better at generating images conditioned on a text prior since it can update the image
	many times during the denoising process. 
	<tr>
		<td>
			<div class="row">
				<div class="column">
					<figure>
					<img src="inputs/orig_dog.png" width=320px>
					<figcaption>Input Image</figcaption>
					</figure>
				</div>
				<div class="column">
					<figure>
					<img src="results/dog_angry.jpg" width=320px>
					<figcaption>Replace dog with angry husky. </figcaption>
					</figure>
				</div>
				</div>
		</td>
	</tr>
	<tr>
		<td>
			<div class="row">
				<div class="column">
					<figure>
					<img src="inputs/orig_dog.png" width=320px>
					<figcaption>Input Image</figcaption>
					</figure>
				</div>
				<div class="column">
					<figure>
					<img src="results/dog_cute.jpg" width=320px>
					<figcaption>Replace dog with cute husky.</figcaption>
					</figure>
				</div>
				</div>
		</td>
	</tr>
	<tr>
		<td>
			<div class="row">
				<div class="column">
					<figure>
					<img src="inputs/orig_dog.png" width=320px>
					<figcaption>Input Image</figcaption>
					</figure>
				</div>
				<div class="column">
					<figure>
					<img src="results/dog_beautiful.jpg" width=320px>
					<figcaption>Replace dog with beautiful husky.</figcaption>
					</figure>
				</div>
				</div>
		</td>
	</tr>
	<tr>
		<td>
			<div class="row">
				<div class="column">
					<figure>
					<img src="inputs/boat.jpeg" width=320px>
					<figcaption>Input Image</figcaption>
					</figure>
				</div>
				<div class="column">
					<figure>
					<img src="results/boat_dog.jpg" width=320px>
					<figcaption>Replace boat with dog.</figcaption>
					</figure>
				</div>
				</div>
		</td>
	</tr>
	<tr>
		<td>
			<div class="row">
				<div class="column">
					<figure>
					<img src="inputs/boat.jpeg" width=320px>
					<figcaption>Input Image</figcaption>
					</figure>
				</div>
				<div class="column">
					<figure>
					<img src="results/boat_dog_colorful.jpg" width=320px>
					<figcaption>Replace boat with colorful dog.</figcaption>
					</figure>
				</div>
				</div>
		</td>
	</tr>
	</table>
  <br>
	<hr>
	<br>

  <table align=center width=850px>
		<center><h1>References</h1></center>
		<tr>
			<td>
        [1] Chang, H., Zhang, H., Jiang, L., Liu, C., and Freeman, W. T., “MaskGIT: Masked Generative Image Transformer”, <i>arXiv e-prints</i>, 2022.
        <br>
        [2] He, K., Gkioxari, G., Dollár, P., and Girshick, R., “Mask R-CNN”, <i>arXiv e-prints</i>, 2017.
        <br>
        [3] Radford, A., “Learning Transferable Visual Models From Natural Language Supervision”, <i>arXiv e-prints</i>, 2021.
        <br>
        [4] Y. Wu, A. Kirillov, F. Massa, W.-Y. Lo, και R. Girshick, ‘Detectron2’, 2019. [Έκδοση σε ψηφιακή μορφή]. Διαθέσιμο στο: https://github.com/facebookresearch/detectron2.
        <br>
        [5] “Common objects in context,” COCO. https://cocodataset.org/#home.
        <br>
        [6] https://www.image-net.org/. 
			</td>

		</tr>
	</table>



	<table align=center width=900px style="margin-top:2em">
		<tr>
			<td width=400px>
				<left>
					This template was originally made by <a href="http://web.mit.edu/phillipi/">Phillip Isola</a> and <a href="http://richzhang.github.io/">Richard Zhang</a> for a <a href="http://richzhang.github.io/colorization/">colorful</a> ECCV project; the code can be found <a href="https://github.com/richzhang/webpage-template">here</a>.
				</left>
			</td>
		</tr>
	</table>

<br>
</body>
</html>

