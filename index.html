<script src="http://www.google.com/jsapi" type="text/javascript"></script> 
<script type="text/javascript">google.load("jquery", "1.3.2");</script>

<script type="text/javascript" id="MathJax-script" async
          src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml.js">
        </script>
        <!-- sets $$ as shorthand for in-text math functions -->
        <script>
         window.MathJax = {
            tex: {
              inlineMath: [['$', '$'], ['\\(', '\\)']]
            }
          };
        </script>

<style type="text/css">
	body {
		font-family: "HelveticaNeue-Light", "Helvetica Neue Light", "Helvetica Neue", Helvetica, Arial, "Lucida Grande", sans-serif; 
		font-weight:300;
		font-size:18px;
		margin-left: auto;
		margin-right: auto;
		width: 1100px;
	}
	
	h1 {
		font-size:32px;
		font-weight:300;
	}
	
	.disclaimerbox {
		background-color: #eee;		
		border: 1px solid #eeeeee;
		border-radius: 10px ;
		-moz-border-radius: 10px ;
		-webkit-border-radius: 10px ;
		padding: 20px;
	}

	video.header-vid {
		height: 140px;
		border: 1px solid black;
		border-radius: 10px ;
		-moz-border-radius: 10px ;
		-webkit-border-radius: 10px ;
	}
	
	img.header-img {
		height: 140px;
		border: 1px solid black;
		border-radius: 10px ;
		-moz-border-radius: 10px ;
		-webkit-border-radius: 10px ;
	}
	
	img.rounded {
		border: 1px solid #eeeeee;
		border-radius: 10px ;
		-moz-border-radius: 10px ;
		-webkit-border-radius: 10px ;
	}
	
	a:link,a:visited
	{
		color: #1367a7;
		text-decoration: none;
	}
	a:hover {
		color: #208799;
	}
	
	td.dl-link {
		height: 160px;
		text-align: center;
		font-size: 22px;
	}
	
	.layered-paper-big { /* modified from: http://css-tricks.com/snippets/css/layered-paper/ */
		box-shadow:
		0px 0px 1px 1px rgba(0,0,0,0.35), /* The top layer shadow */
		5px 5px 0 0px #fff, /* The second layer */
		5px 5px 1px 1px rgba(0,0,0,0.35), /* The second layer shadow */
		10px 10px 0 0px #fff, /* The third layer */
		10px 10px 1px 1px rgba(0,0,0,0.35), /* The third layer shadow */
		15px 15px 0 0px #fff, /* The fourth layer */
		15px 15px 1px 1px rgba(0,0,0,0.35), /* The fourth layer shadow */
		20px 20px 0 0px #fff, /* The fifth layer */
		20px 20px 1px 1px rgba(0,0,0,0.35), /* The fifth layer shadow */
		25px 25px 0 0px #fff, /* The fifth layer */
		25px 25px 1px 1px rgba(0,0,0,0.35); /* The fifth layer shadow */
		margin-left: 10px;
		margin-right: 45px;
	}

	.paper-big { /* modified from: http://css-tricks.com/snippets/css/layered-paper/ */
		box-shadow:
		0px 0px 1px 1px rgba(0,0,0,0.35); /* The top layer shadow */

		margin-left: 10px;
		margin-right: 45px;
	}


	.layered-paper { /* modified from: http://css-tricks.com/snippets/css/layered-paper/ */
		box-shadow:
		0px 0px 1px 1px rgba(0,0,0,0.35), /* The top layer shadow */
		5px 5px 0 0px #fff, /* The second layer */
		5px 5px 1px 1px rgba(0,0,0,0.35), /* The second layer shadow */
		10px 10px 0 0px #fff, /* The third layer */
		10px 10px 1px 1px rgba(0,0,0,0.35); /* The third layer shadow */
		margin-top: 5px;
		margin-left: 10px;
		margin-right: 30px;
		margin-bottom: 5px;
	}
	
	.vert-cent {
		position: relative;
		top: 50%;
		transform: translateY(-50%);
	}
	
	hr
	{
		border: 0;
		height: 1px;
		background-image: linear-gradient(to right, rgba(0, 0, 0, 0), rgba(0, 0, 0, 0.75), rgba(0, 0, 0, 0));
	}

  figure 
  {
    display: inline-block;
  }

  figure figcaption 
  {
    border: 1px;
    text-align: center;
  }

  .column {
            float: left;
            height: auto;
            padding: 5px;
        }

</style>

<html>
<head>
	<title>OpenGIT</title>
	<meta property="og:image" content="Path to my teaser.png"/> <!-- Facebook automatically scrapes this. Go to https://developers.facebook.com/tools/debug/ if you update and want to force Facebook to rescrape. -->
	<meta property="og:title" content="Creative and Descriptive Paper Title." />
	<meta property="og:description" content="Paper description." />

	<!-- Get from Google Analytics -->
	<!-- Global site tag (gtag.js) - Google Analytics -->
	<script async src=""></script> 
	<script>
		window.dataLayer = window.dataLayer || [];
		function gtag(){dataLayer.push(arguments);}
		gtag('js', new Date());

		gtag('config', 'UA-75863369-6');
	</script>
</head>

<body>
	<br>
	<center>
		<span style="font-size:36px">Open-Domain Compositional Image Editing with Text</span>
		<table align=center width=600px>
			<table align=center width=600px>
				<tr>
					<td align=center width=100px>
						<center>
							<span style="font-size:24px"><a href="https://en.wikipedia.org/wiki/James_J._Gibson">Zhizhuo Zhou</a></span>
						</center>
					</td>
          <td align=center width=100px>
						<center>
							<span style="font-size:24px"><a href="https://en.wikipedia.org/wiki/James_J._Gibson">Gaoyue Zhou</a></span>
						</center>
					</td>
					<td align=center width=100px>
						<center>
							<span style="font-size:24px"><a href="https://www.linkedin.com/in/qiyun-chen-313a16193/">Qiyun Chen</a></span>
						</center>
					</td>
					
				</tr>
			</table>
			<table align=center width=250px>
				<!-- <tr>
					<td align=center width=120px>
						<center>
							<span style="font-size:24px"><a href=''>[Paper]</a></span>
						</center>
					</td>
					<td align=center width=120px>
						<center>
							<span style="font-size:24px"><a href='https://github.com/richzhang/webpage-template'>[GitHub]</a></span><br>
						</center>
					</td>
				</tr> -->
			</table>
		</table>
	</center>

	<center>
		<table align=center width=850px>
			<tr>
				<td width=260px>
					<center>
						<img class="round" style="width:600px" src="./resources/teaser.png"/>
					</center>
				</td>
			</tr>
		</table>
		<table align=center width=850px>
			<tr>
				<td>
					This is a final course project for <a href="https://learning-image-synthesis.github.io/sp22/">16-726 Learning Based Image Synthesis</a> in Spring 2022</a>.
				</td>
			</tr>
		</table>
	</center>

	<hr>

	<table align=center width=850px>
		<center><h1>Abstract</h1></center>
		<tr>
			<td>
				Building on top of a recent class conditional generative image transformer, <a href="https://arxiv.org/abs/2202.04200">MaskGIT</a>, we propose to develop a pipeline for language controlled image editing. We focus 
				on a simple yet ubiquitous logic, {a} to {b}, which can be expressed as "change {a} to {b}" or "replace {a} with {b}", and apply the desired actions by performing edits on 
				an input image. We constrain {a} to be a class in <a href="https://cocodataset.org/">COCO</a> and {b} to be an open-domain phrase that describes an object. 
        Finally, we leverage <a href="https://arxiv.org/abs/1703.06870">Mask R-CNN</a>, <a href="https://arxiv.org/abs/2202.04200">MaskGIT</a>, 
        and <a href="https://openai.com/blog/clip/">CLIP</a> to perform region targeted class conditional image generation
        and image-language optimization.
			</td>
		</tr>
	</table>

	<br>
	<hr>
	
	<!-- <table align=center width=850px>
		<center><h1>Related Works </h1></center>
		<tr>
			<td>
				Recent works in learning based synthesis show that a two stage VQ-VAE and transformer setup is able to generate realistic and diverse images. 
        
			</td>
		</tr>
	</table> -->

	<!-- <br>
	<hr>
	<br> -->

	<table align=center width=850px>
		<center><h1>Method</h1></center>
		<tr>
      <td>
        <center>
          <img class="round" style="width:800px" src="./resources/pipeline.png"/>
          <figcaption>OpenGIT Pipeline</figcaption>
        </center>
			</td>
		</tr>
	</table>

  <table align=center width=850px>
    <tr>
      <td>
        <br>
        Our proposed OpenGIT Pipeline consists of three main components: <b>Natural Language Processing Module</b>,
        <b>Class Conditional Image Generation</b>, and <b>CLIP Guided Optimization</b>.<br><br>
        The entire procedure is as follows: <br><br>
        Firstly, OpenGIT performs Natural Language Processing (NLP), the user inputs an image and a natural language prompt
        specifying the source category (the item to be replaced in the input image) and the target category (
        the item to change into). The source category must be from one of the 
        <a href="https://github.com/amikelive/coco-labels"> categories in the COCODataset </a>. The main target
        category (or categories) should be within the 1000 lables in the <a href="https://www.image-net.org/">ImageNet Dataset</a>. 
        However, it can be a simple category like "husky", or it can also be any open-domain phrase that describes
        the category (categories), such as "zebra-striped cat" or "fish and strawberry".<br>
        As an illustration for the first module, the user could input an image of a dog and a prompt 
        "Replace dog with lemon husky". Then, our module would parse "dog" as our source category, "husky"
        as our target category, and "lemon husky" as our CLIP prompt. Here, note that because we used a descriptive
        word for our target category, we would use CLIP to further optimize our image. Otherwise, if the input prompt
        merely said "Replace dog with husky", then we would not have any CLIP prompt nor perform CLIP optimization.
        <br><br>
        Secondly, OpenGIT performs Class Conditional Generation. This module take as inputs
        the input image along with the source and target category. The module first uses Detectron2 and Mask R-CNN
        to locate the bounding box of the source category, and then feed it through a pretrained MaskGIT
        model to generate a class conditioned image. Here, we offer users to choose a hyperparameter 
        $r \in (0, 1]$ as the scale of the original bounding box. For example, in general, if $r = 1$, 
        then OpenGIT would completely replace the source category with the target category. If $r < 1$, 
        then OpenGIT would keep the outerior of the source category and merges the target category into the 
        source category (see examples in Results section below).
        <br><br>
        Thirdly, OpenGIT performs CLIP Guided Optimization. For the user input prompts with a descriptive
        prompt for the target category, the first NLP module would obtain a non-empty CLIP prompt.
        We take our class conditioned generated output and the CLIP prompt as the input to this
        module, and we use a pretrained CLIP module to generate the output image.
      </td>
    </tr>
  </table>

  <br>
  <hr>
  <br>

  <table align=center width=850px>
		<center><h1>Modules</h1></center>
		<tr>
			<td>
				<h3>Object Detection</h3>
				<center>
					<img class="round" style="width:800px" src="./resources/m1_maskrcnn.png"/>
					<figcaption>OpenGIT Pipeline</figcaption>
				</center>
				<p>TODO</p>
			</td>
		</tr>
		<tr>
			<td>
				<h3>Class-Conditioned Image Editing</h3>
				<center>
					<img class="round" style="width:800px" src="./resources/m2_maskgit.png"/>
					<figcaption>OpenGIT Pipeline</figcaption>
				</center>
				<p>TODO</p>
			</td>
		</tr>
		<tr>
			<td>
				<h3>Fine-tuning with Text</h3>
				<center>
					<img class="round" style="width:800px" src="./resources/m3_clip.png"/>
					<figcaption>OpenGIT Pipeline</figcaption>
				</center>
				<p>TODO</p>
			</td>
		</tr>
	</table>

  

	<br>
	<hr>
	<br>

	<table align=center width=850px>
		<center><h1>Ablation</h1></center>
		<tr>
			<td>
				<h3>The Effect of Bounding Box Size</h3>
				<p>TODO</p>
				<table align=center width=850px>
					<div class="row">
					  <div class="column">
						<figure>
						  <img src="results/bbox_ratio/original_05-08-12-15-47_t332_happy_r0.4.jpeg" height=120px>
						  <figcaption>User Input Image: cat</figcaption>
						</figure>
					  </div>
					  <div class="column">
						<figure>
						  <img src="results/bbox_ratio/bbox04.png" height=120px>
						  <figcaption>Bounding box ratio = 0.4</figcaption>
						</figure>
					  </div>
					  <div class="column">
						<figure>
						  <img src="results/bbox_ratio/before-clip_05-08-12-15-47_t332_happy_r0.4.jpg" height=120px>
						  <figcaption>Output </figcaption>
						</figure>
					  </div>
					</div>
		  
					<div class="row">
					  <div class="column">
						<figure>
						  <img src="results/bbox_ratio/original_05-08-12-19-27_t332_happy_r1.jpeg" height=120px>
						  <figcaption>User Input Image: cat</figcaption>
						</figure>
					  </div>
					  <div class="column">
						<figure>
						  <img src="results/bbox_ratio/bbox1.png" height=120px>
						  <figcaption>Bounding box ratio =   1</figcaption>
						</figure>
					  </div>
					  <div class="column">
						<figure>
						  <img src="results/bbox_ratio/before-clip_05-08-12-18-40_t332_happy_r1.jpg" height=120px>
						  <figcaption>Output</figcaption>
						</figure>
					</div>
		  
					
				</div>
				</table>
				

			</td>
		</tr>
	</table>

  

	<br>
	<hr>
	<br>

	<table align=center width=850px>
		<center><h1>Results</h1></center>
		<tr>
			<td>
				Here're the results of our OpenGIT pipeline. 
        Recall that the user inputs an image of an object with a prompt 
        similar to "Replace {a} with {b}".<br><br>
        Firstly, we present the image generation for the case where {b} is simply a target category, 
        thus no CLIP optimization is performed.
        <table align=center width=850px>
          <div class="row">
            <div class="column">
              <figure>
                <img src="inputs/orig_dog.png" width=320px>
                <figcaption>User Input Image: dog</figcaption>
              </figure>
            </div>
            <div class="column">
              <figure>
                <img src="results/16726_dog_leopard.jpg" width=320px>
                <figcaption>Replace dog with leopard </figcaption>
              </figure>
            </div>
          </div>

          <div class="row">
            <div class="column">
              <figure>
                <img src="inputs/keyboard.jpeg" width=320px>
                <figcaption>User Input Image: keyboard</figcaption>
              </figure>
            </div>
            <div class="column">
              <figure>
                <img src="results/16726_keyboard_cat.png" width=320px>
                <figcaption>Prompt: Replace keyboard with cat</figcaption>
              </figure>
            </div>
            <div class="column">
              <figure>
                <img src="inputs/dog.jpeg" width=320px>
                <figcaption>User Input Image: dog</figcaption>
              </figure>
            </div>
            <div class="column">
              <figure>
                <img src="results/16726_dog_husky.jpg" width=320px>
                <figcaption>Replace dog with husky</figcaption>
              </figure>
            </div>
          </div>

          <div class="column">
            <figure>
              <img src="inputs/boat.jpeg" width=320px>
              <figcaption>User Input Image: boat</figcaption>
            </figure>
          </div>
        </div>

        <div class="column">
          <figure>
            <img src="results/16726_boat_loaf.jpg" width=320px>
            <figcaption>Replace dog with tigercat</figcaption>
          </figure>
        </div>
      </div>
      </table>
      
      Next, we present the image generation for the case where {b} is a descriptive phrase
      about a target category, we perform CLIP optimization in this case.

      <table align=center width=850px>
        <div class="row">
          <div class="column">
            <figure>
              <img src="inputs/orig_dog.png" width=320px>
              <figcaption>User Input Image: dog</figcaption>
            </figure>
          </div>
          <div class="column">
            <figure>
              <img src="results/purple_butterfly_leopard.png" width=320px>
              <figcaption>Replace dog with purple butterfly leopard</figcaption>
            </figure>
          </div>
        </div>

          <div class="column">
            <figure>
              <img src="inputs/keyboard.jpeg" width=320px>
              <figcaption>User Input Image: keyboard</figcaption>
            </figure>
          </div>
          <div class="column">
            <figure>
              <img src="results/keyboard_zebra_cat.jpg" width=320px>
              <figcaption>Prompt: Replace keyboard with zebra-striped cat</figcaption>
            </figure>
          </div>
          <div class="column">
            <figure>
              <img src="inputs/dog.jpeg" width=320px>
              <figcaption>User Input Image: dog</figcaption>
            </figure>
          </div>
          <div class="column">
            <figure>
              <img src="results/dog_rainbow_husky.jpg" width=320px>
              <figcaption>Replace dog with rainbow husky</figcaption>
            </figure>
          </div>
        </div>

        <div class="column">
          <figure>
            <img src="inputs/boat.jpeg" width=320px>
            <figcaption>User Input Image: boat</figcaption>
          </figure>
        </div>
      </div>

      <div class="column">
        <figure>
          <img src="results/16726_boat_loaf.jpg" width=320px>
          <figcaption>Replace boat with cinnamon roll</figcaption>
        </figure>
      </div>
    </div>
    </table>

    Lastly, we present the results of {b} where b is a combination of different categories.
    Essentially, we achieve the results via combining the latent features of different categories.
    <table align=center width=850px>
      <div class="row">
        <div class="column">
          <figure>
            <img src="inputs/orig_dog.png" width=320px>
            <figcaption>User Input Image: dog</figcaption>
          </figure>
        </div>
        <div class="column">
          <figure>
            <img src="results/dog_fish_n_strawberry.png" width=320px>
            <figcaption>Replace dog with purple butterfly leopard</figcaption>
          </figure>
        </div>
      </div>
    </table>


			</td>
		</tr>
	</table>
  
  <br>
	<hr>
	<br>

  <table align=center width=850px>
		<center><h1>References</h1></center>
		<tr>
			<td>
        [1] Chang, H., Zhang, H., Jiang, L., Liu, C., and Freeman, W. T., “MaskGIT: Masked Generative Image Transformer”, <i>arXiv e-prints</i>, 2022.
        <br>
        [2] He, K., Gkioxari, G., Dollár, P., and Girshick, R., “Mask R-CNN”, <i>arXiv e-prints</i>, 2017.
        <br>
        [3] Radford, A., “Learning Transferable Visual Models From Natural Language Supervision”, <i>arXiv e-prints</i>, 2021.
        <br>
        [4] Y. Wu, A. Kirillov, F. Massa, W.-Y. Lo, και R. Girshick, ‘Detectron2’, 2019. [Έκδοση σε ψηφιακή μορφή]. Διαθέσιμο στο: https://github.com/facebookresearch/detectron2.
        <br>
        [5] “Common objects in context,” COCO. https://cocodataset.org/#home.
        <br>
        [6] https://www.image-net.org/. 
			</td>

		</tr>
	</table>



	<table align=center width=900px style="margin-top:2em">
		<tr>
			<td width=400px>
				<left>
					This template was originally made by <a href="http://web.mit.edu/phillipi/">Phillip Isola</a> and <a href="http://richzhang.github.io/">Richard Zhang</a> for a <a href="http://richzhang.github.io/colorization/">colorful</a> ECCV project; the code can be found <a href="https://github.com/richzhang/webpage-template">here</a>.
				</left>
			</td>
		</tr>
	</table>

<br>
</body>
</html>

